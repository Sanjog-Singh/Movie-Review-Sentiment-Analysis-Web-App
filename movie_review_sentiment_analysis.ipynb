{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82fd81-2f20-4414-a8fb-27fef6239d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0949b6f-aa92-409c-920b-c2ca70616829",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07bbf5f-5b5d-46b8-989d-b0c3aa46c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For converting reviews in a lower case alphabet\n",
    "df['review']=df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3ebf43-92c6-4ecc-b1b3-8b2a14af84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951f323d-ef72-407f-b778-e7a66806fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re stands for Regular Expressions which allows you to search, match, and replace patterns in strings for text processing tasks.\n",
    "import re\n",
    "def removetags(df, column):\n",
    "\n",
    "    def cleanhtml(text):\n",
    "        clean = re.compile(r'<.*?>')\n",
    "        return re.sub(clean, '', text)\n",
    "    \n",
    "    df[column] = df[column].astype(str)\n",
    "    df[column] = df[column].apply(cleanhtml)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb2ae3-8dd9-4598-9285-28fdeafb1006",
   "metadata": {},
   "outputs": [],
   "source": [
    "removetags(df,'review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28812692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0410939-ca9f-471f-a697-2dafdb2be329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For removing the punctuations\n",
    "import string\n",
    "def rpunt(df,column):\n",
    "    translator=str.maketrans('','',string.punctuation)\n",
    "    df[column]=df[column].str.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee66e3a3-c7f7-424e-a90a-56a261fc081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpunt(df,'review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c5c98-5437-4373-985a-209cb335cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d647a3-8982-4bed-b11e-6d4f668c8c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek You (also a chat program)\",\n",
    "    \"ILU\": \"I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laugh My A.. Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A..\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA?\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your Sex And Age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laugher\",\n",
    "    \"TFW\": \"That Feeling When\",\n",
    "    \"MFW\": \"My Face When\",\n",
    "    \"MRW\": \"My Reaction When\",\n",
    "    \"IFYP\": \"I Feel Your Pain\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"TNTL\": \"Trying Not To Laugh\",\n",
    "    \"JK\": \"Just Kidding\",\n",
    "    \"IDC\": \"I Don’t Care\",\n",
    "    \"ILY\": \"I Love You\",\n",
    "    \"IMU\": \"I Miss You\",\n",
    "    \"ADIH\": \"Another Day In Hell\",\n",
    "    \"ZZZ\": \"Sleeping, Bored, Tired\",\n",
    "    \"WYWH\": \"Wish You Were Here\",\n",
    "    \"TIME\": \"Tears In My Eyes\",\n",
    "    \"BAE\": \"Before Anyone Else\",\n",
    "    \"FIMH\": \"Forever In My Heart\",\n",
    "    \"BSAAW\": \"Big Smile And A Wink\",\n",
    "    \"BWL\": \"Bursting With Laughter\",\n",
    "    \"BFF\": \"Best Friends Forever\",\n",
    "    \"CSL\": \"Can’t Stop Laughing\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14996d6-730e-4af2-b87e-5690b8c06516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To replace the abbreviations with it's full form\n",
    "def correctwor(df,column):\n",
    "    newt=[]\n",
    "    for text in df[column]:\n",
    "        neww=[]\n",
    "        for word in text.split():\n",
    "            if  word.upper in abbreviations:\n",
    "                neww.append(abbreviations[word.upper])\n",
    "            else:\n",
    "                neww.append(word)\n",
    "            newt.append(' '.join(neww))\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d42b9a-b387-4dd5-8294-072be288eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "correctwor(df,'review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400b1d1f-440c-4134-b3f0-07f2d0b439ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a0d88-949a-484b-802b-374784fb6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For removing stopwords of english language\n",
    "def remove_sw(df,column):\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    new_te=[]\n",
    "    for text in df[column]:\n",
    "        neww=[]\n",
    "        for word in text.split():\n",
    "            if word.lower() not in stop_words:\n",
    "                neww.append(word)\n",
    "        clt=' '.join(neww)\n",
    "        new_te.append(clt)\n",
    "        \n",
    "    df[column]=new_te\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a0d819-721c-401c-ab95-c0dbb670a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_sw(df,'review')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f883bf4c-e9c4-4b2e-96d5-778f753e8d9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For performing Tokenization\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f200e3d-9587-4012-abbd-8f1d71bcf7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For performing Tokenization which means breaking text into individual words\n",
    "def toke(df,column):\n",
    "    df[column]=df[column].astype(str)\n",
    "    def tokentext(text):\n",
    "        doc=nlp(text)\n",
    "        tokens=[]\n",
    "        for token in doc:\n",
    "            tokens.append(token.text)\n",
    "        return tokens\n",
    "    newt=[]\n",
    "    for text in df[column]:\n",
    "        newt.append(tokentext(text))\n",
    "    df[\"Tokens\"]=newt\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2216f89-0b9c-4695-9ba8-224be63393e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "toke(df,'review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f546b-24ab-4ee3-833f-089e97766353",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9386d3e-70a0-4cc4-92a3-0f0cbbb8a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"clean.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e2dd5-8b53-49cd-96c3-48032a5a04dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a collection of tokens in the form of a list\n",
    "def corpus(df,column):\n",
    "    single_corpus=[]\n",
    "    for tokenl in df[column]:\n",
    "        for token in tokenl:\n",
    "            single_corpus.append(token)\n",
    "    return single_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca248cf6-5075-409c-9503-978a704aa344",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_corpus=corpus(df,'Tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d4c70c-4524-469d-87af-2220ba4c284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpuslength=len(whole_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624180c0-00fd-4d71-998f-3f13d3ce4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=set(whole_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142030c9-b567-4b5f-a9f1-de6e97a13463",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabularylength=len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25197f7c-18ff-4327-a9e9-abc92ccc2641",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'corpus is {corpuslength} ,vocabulary is {vocabularylength}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374902f5-1798-497b-8417-f362ee70f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joins tokens into a single string\n",
    "df['Tokens'] = df['Tokens'].apply(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787ed70-9824-487a-8a9a-7bb4f89391ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the variables for model training\n",
    "X = df['Tokens']  \n",
    "y = df['sentiment']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9703ec8c-de2d-4c4f-8210-adad82d3755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a7a3a4-0136-469a-bd92-fc69be8b9e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training and testing of the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9080d8f5-4eb6-40f0-9dfb-ff5ff247db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert tokens into vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "vectorizer = TfidfVectorizer(max_df=0.9, min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b72280c-dfd3-4252-9e34-fe7f666da6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2487f37-d201-4e3b-89d3-0cc6e2ed25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeea80e0-f15d-4d02-b6a0-f61c7da0a78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66580658-9ef3-4baa-9047-055c391f34a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598eeed3-25ac-47c8-a541-e2db1764eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb0b6b-9dda-482d-ac26-314827b3e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For calculating accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c99f61a-8035-4ccd-b8e9-0bc5f5ab64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c803a1f7-1fc4-4e2d-a931-034b52569ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_review = \"The movie was fantastic! I loved the performances.\"\n",
    "new_review_tfidf = vectorizer.transform([new_review])\n",
    "new_prediction = model.predict(new_review_tfidf)\n",
    "new_prediction_label = 'positive' if new_prediction == 1 else 'negative'\n",
    "\n",
    "print(f\"Review: {new_review}\")\n",
    "print(f\"Prediction: {new_prediction_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941ff54-bcda-45ee-88dd-78978f15c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joblib is a Python library used to save ml models to a file so that you can later load and reuse them\n",
    "import joblib\n",
    "joblib.dump(model, 'sentiment_model.pkl')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb17c8d-0c03-4624-a838-19aa8d803e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log loss is a metric used in ml to evaluate the performance of binary classification problems\n",
    "from sklearn.metrics import log_loss\n",
    "y_pred_proba = model.predict_proba(X_test_tfidf)\n",
    "loss = log_loss(y_test, y_pred_proba)\n",
    "print(f\"Log Loss (Binary Cross-Entropy): {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
